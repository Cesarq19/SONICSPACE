# Project Submission - Vision Ears

Team Name: Sonic Coders 

Application Name: Vision Ears

### Summary of the High Level Project
We developed a multimedia app that incorporates content such as images, videos and musical compositions provided by NASA. As it is easy to interact with the user, it becomes a valuable tool to promote interest in astronomy and space research in children, young people and adults around the world. The challenge was solved by developing a Python code that allows transforming the videos captured by NASA's space telescopes into audios; considering characteristics such as frequency and intensity of luminosity. This figma mockup presents several sections to observe and learn from space; such as videos, videos with sounds and sounds with sensory experience. In addition, we made a prototype case out of latex phone material, so that the waves and vibrations provide the user with a sensory experience of the cosmos that involves more than one sense of their body. We are committed to ensuring that the cosmos can be understood by everyone, so our app has been specially designed so that blind and visually impaired people can also be included as space travellers or scientists.

### Link to Final Project

**GitHub**
https://github.com/Cesarq19/lavate-las-manos/edit/main/README.md

**Figma**
https://www.figma.com/file/tC4E8FeA7Xk3TIsuOcvRiK/NS-Mockup?type=design&node-id=0%3A1&mode=design&t=mikdSZvvZyDXXFnC-1

**CAD Prototype**
https://grabcad.com/library/touch-cell-phone-case-1

### Link to Project "Demo"

    **drive**
    https://drive.google.com/drive/folders/1Z2zEkCf5_MchIVIHuztZYjNF0IZw8Y_y?usp=drive_link

### Detailed Project Description

- **What exactly does it do?**

The project consists of several aspects: firstly, access to the information provided by NASA and the addition of contexts to make it more feasible to report on the data; secondly, sonification, which allows the videos captured by NASA to be converted into sound for the visually impaired; and finally, a sensory experience through vibrations and oscillations.

- **How does it work?**

Its operation depends on the option that is chosen within the app, it is given in the manual option and enters the different types of existing options, being the options, videos with information, sounds of the videos and the sensory with vibrations for a better experience.

- **What benefits does it have?**

This brings many benefits to the scientific community, as there will be places whose sounds are difficult to identify for the human ear and even for current mobile devices, but it can give a meaningful idea of what the sound of those videos would be like, as well as helping visually impaired people as the app not only stimulates sight, but also hearing and touch. We propose an interactive tool to get children interested in space science and become the next scientists or space travellers.

- **What do you hope to achieve?**

We hope that less fortunate people with visual impairments will be able to appreciate the wonders of the universe, so that the scientific community will grow and have a desire for research and work of this kind.

- **What tools, coding languages, hardware or software did you use to develop your project?**

We use programming languages such as python for the programming of the code that performs the sonification from the database and some software such as inventor for the design of the prototype phone case and figma for the development of the mockup.

---

### Space Agency Data

We used the data provided by NASA about sonifications of images and videos, for example, "NASA's previous sonifications in 2D", which helped us to have a notion of the approach we would have to use to solve this challenge, since it helps us to see previous sonification models and we can implement those that are more feasible. 
In addition to this, we used some space video pages such as "3D representations of flybys and flybys based on Hubble Space Telescope images" and "Hyperspectral data from NASA's EO-1 Hyperion instrument", which helped us to pigeonhole and classify the sounds based on various aspects such as magnitude of brightness intensity and contrasts in terms of seconds.
Finally, we learned a little more through space programs about the experiences that astronauts live in space through the 5 senses and associating them to what can be lived with this new application.

---

### Hackathon Journey

Our experience in this Hackathon was really surprising, exciting and above all quite learning. We learned more about the sonification of images and videos by extracting a database that does not always exist as it happened on this occasion. What inspired us most to choose this challenge was the impact it can have on society, not only to encourage interest in these kinds of issues, but also to address the problems that commonly exist with people who have a disability. 
For this challenge we analysed the data and understood how useful it can be for people who are visually impaired or even have no vision, through sound you can generate experiences similar to the visualisations of a video and with the sensory ability to feel the vibrations and make the user experience more real, leaving as a message that there is no restriction to generate an interest and spatial love. Each part of the challenge we faced was tackled as a team and not each one on their own, as by collaborating with each other the result was better and we were able to project our message to people.
Finally, we would like to thank the mentors who were in our environment, with whom we were able to socialise and clear certain doubts that arose during the challenge and that could have further complicated our proposed solution.

---

### Reference - Data

- **Sonifications - NASA Science. (s. f.).**
https://science.nasa.gov/mission/hubble/multimedia/sonifications
- **Videos. (s. f.). Hubble.**
https://hubblesite.org/videos?Type=Scientific%20Visualizations&keyword=flight
- **Survey, U.-. U. G. (s. f.). EarthExplorer. None.**
https://earthexplorer.usgs.gov/
